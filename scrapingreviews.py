# -*- coding: utf-8 -*-
"""ScrapingReviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QEMbG8Bcob6g24NBnUxvy7U55PvZom6
"""

# !pip install selenium requests beautifulsoup4

# !apt-get update

# !apt-get install -y chromium-browser

# !apt install chromium-chromedriver

import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# options for setting chromium on collab
options = webdriver.ChromeOptions()
options.add_argument('--no-sandbox')
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--disable-dev-shm-usage')
options.binary_location = '/usr/bin/chromium-browser/chromedriver'

def scroll_down(driver, num_scrolls):
    for _ in range(num_scrolls):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

def scrape_reviews(url, num_scrolls):
    # Start a headless browser session
    driver = webdriver.Chrome(options=options) # options for setting chromium on collab
    # driver = webdriver.Chrome()  # for running on local machine
    driver.get(url)

    try:
        # Wait for the review container to load
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "apphub_CardTextContent")))

        # Scroll down a specific number of times
        scroll_down(driver, num_scrolls)

        # Once scrolled, get the page source
        page_source = driver.page_source

        # Parse the page source with BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')

        # Find all review elements
        reviews = soup.find_all('div', class_='apphub_CardTextContent')

        # Create a list to store review data
        review_list = []

        for review in reviews:
            date_posted = review.find('div', class_='date_posted').text.strip()
            review_content = review.get_text(separator='\n').strip()
            review_list.append({
                "date_posted": date_posted,
                "review_content": review_content
            })

        print(str(len(review_list)) + " reviews scrapped!!\n")

        # Store the review data in a JSON file
        with open('reviews.json', 'w', encoding='utf-8') as file:
            json.dump(review_list, file, ensure_ascii=False, indent=4)

        print("reviews added to reviews.json file!!\n")

    finally:
        # Close the browser session
        driver.quit()

url = "https://steamcommunity.com/app/578080/reviews/?browsefilter=toprated&snr=1_5_100010_#"
num_scrolls = 20  # Scroll down 20 times
scrape_reviews(url, num_scrolls)

